from pyspark.sql.functions import col, lit

def get_narrowband_sectors(prb_vision_start, prb_vision_end, alptCoreDB, elptCoreDB, interf_level, sam_data_present, spark, markets=[]):
    sam_sectors = None

    if sam_data_present:
        sam_sectors = get_hot_samsung_sectors_sparkdf(prb_vision_start, prb_vision_end, alptCoreDB, interf_level, markets)
        broken_sam = get_hot_samsung_sectors_sparkdf(prb_vision_start, prb_vision_end, elptCoreDB, interf_level, markets)
    else:
        broken_sam = spark.createDataFrame([], get_hot_samsung_sectors_sparkdf.schema)  # or use an empty DataFrame with same schema

    broken_nok = get_hot_nokia_sectors_sparkdf(prb_vision_start, prb_vision_end, alptCoreDB, interf_level, markets)
    broken_eric = get_hot_ericsson_sectors_sparkdf(prb_vision_start, prb_vision_end, elptCoreDB, interf_level, markets)

    broken_branches = broken_sam.union(broken_nok).union(broken_eric)

    # Add VENDOR column for each and union
    eric_sectors = broken_eric.withColumn("VENDOR", lit("Ericsson"))
    vendor_sectors = [eric_sectors]

    if sam_data_present:
        sam_sectors = sam_sectors.withColumn("VENDOR", lit("Samsung")).withColumn("rfbranchrx", lit(1))
        vendor_sectors.append(sam_sectors)

    nok_sectors = broken_nok.withColumn("VENDOR", lit("Nokia"))
    vendor_sectors.append(nok_sectors)

    # Union all vendor sectors
    all_sectors = vendor_sectors[0]
    for sect in vendor_sectors[1:]:
        all_sectors = all_sectors.union(sect)

    all_sectors = all_sectors.withColumn("day_copy", col("day")) \
                             .withColumn("hr", col("hr")) \
                             .dropDuplicates(subset=["ENODEB", "SECTORCARRIERREF"])

    # Scoring
    sector_ref_copy = all_sectors.select("branchrx", "day", "hr").withColumnRenamed("branchrx", "secbranchrx")
    df_predictions = all_sectors.drop("day_copy", "SECTORCARRIERREF", "VENDOR") \
        .join(sector_ref_copy, on=["ENODEB", "hr", "day"]) \
        .applyInPandas(create_and_score_prb_image, schema=narrowband_schema)

    # Efficient casting
    cols_to_cast = ["nb", "nb_count", "hr", "nb_hour", "nb_pred"]
    for col_name in cols_to_cast:
        df_predictions = df_predictions.withColumn(col_name, col(col_name).cast("int"))

    df_predictions = df_predictions.orderBy("SECTORCARRIERREF", "ENODEB", "end_hour", "nb_count", ascending=False) \
                                   .dropDuplicates(subset=["ENODEB", "SECTORCARRIERREF", "VENDOR", "day", "hr"])

    return all_sectors, df_predictions, broken_branches


df_predictions = (
    df_predictions
    .withColumn("nb", col("nb") == "True")  # original logic: create boolean column
    .withColumn("start_hour", col("start_hour").cast("int"))
    .withColumn("end_hour", col("end_hour").cast("int"))
    .withColumn("start_prb", col("start_prb").cast("int"))
    .withColumn("end_prb", col("end_prb").cast("int"))
    .withColumn("nb_count", col("nb_count").cast("int"))
    .select("SECTORCARRIERREF", "ENODEB", "start_hour", "end_hour", "start_prb", "end_prb", "nb", "nb_count")
    .dropDuplicates(["ENODEB", "SECTORCARRIERREF", "VENDOR", "start_hour", "end_hour", "start_prb", "end_prb", "nb"])
)

new 

# Line 275 onward — keep logic same, just optimize structure

if sam_sectors is not None:
    sam_sectors = sam_sectors.withColumn("VENDOR", lit("Samsung")) \
                             .withColumn("frbbranchr", lit(""))

    nok_sectors = nok_sectors.withColumn("VENDOR", lit("Nokia")) \
                             .withColumn("frbbranchr", lit(""))

    all_sectors = nok_sectors.select(erc_cols).union(sam_sectors.select(erc_cols)).union(all_sectors)

all_sectors = all_sectors.cache()

# Add helper columns
all_sectors = all_sectors.withColumns({
    "day_copy": col("day"),
    "seccarr_ref_copy": col("SECTORCARRIERREF")
})

# Drop duplicates and unnecessary column before scoring
all_sectors = all_sectors.dropDuplicates(["ENODEB", "SECTORCARRIERREF", "VENDOR", "frbbranchr", "day", "hr"]) \
                         .drop("WEIGHTEDAVGRADIOINTERFERENCE")

# Score using pandas UDF
df_predictions = all_sectors.groupby("day_copy", "VENDOR", "frbbranchr", "seccarr_ref_copy") \
                            .applyInPandas(create_and_score_prb_image, schema=narrowband_schema)

# Filter for nb == 'True' and cast types
df_predictions = df_predictions.filter(col("nb") == "True") \
    .withColumn("nb_count", col("nb_count").cast("int")) \
    .withColumn("start_hour", col("start_hour").cast("int")) \
    .withColumn("end_hour", col("end_hour").cast("int")) \
    .withColumn("start_prb", col("start_prb").cast("int")) \
    .withColumn("end_prb", col("end_prb").cast("int"))

# Final formatting and deduplication
df_predictions = df_predictions.select(
    "SECTORCARRIERREF", "ENODEB", "start_hour", "end_hour",
    "start_prb", "end_prb", "nb", "nb_count"
).orderBy("start_hour", "end_hour", "start_prb", "end_prb", ascending=False) \
 .dropDuplicates(["ENODEB", "SECTORCARRIERREF", "VENDOR"])

# Drop temporary columns from all_sectors
all_sectors = all_sectors.drop("day_copy", "seccarr_ref_copy")

# Return final results
return all_sectors, df_predictions, broken_branches

new opt

sc = get_sectorcarrierref(date, epltCoreDB, spark).alias("sc")

# Define join condition for readability
join_cond = [
    df_hrly_interference_all["ENODEB"] == sc["ENODEB"],
    df_hrly_interference_all["SECTORCARRIERREF"] == sc["SECTORCARRIERREF"]
]

df_hrly_interference_all = df_hrly_interference_all \
    .join(sc, join_cond, how='left') \
    .withColumnRenamed("SECTOR", "EUTRANCELL") \
    .drop(*["ENODEB", "SECTORCARRIERREF", "DAY"])


from pyspark.sql.functions import col, split, when

vendor_condition = col("Vendor").isin("Samsung", "Nokia")
split_col = split(col("SECTORCARRIERREF"), "-")

df_hrly_interference_all = df_hrly_interference_all \
    .withColumn("EUTRANCELL", when(vendor_condition, split_col[0]).otherwise(col("EUTRANCELL"))) \
    .withColumn("CARRIER", when(vendor_condition, split_col[1]).otherwise(col("CARRIER")))


from pyspark.sql.functions import col, split, when

# Join with renamed column and drop unneeded columns
df_hourly_prb = df_hourly_prb.join(
    sc, 
    (df_hourly_prb["ENODEB"] == sc["ENODEB"]) & 
    (df_hourly_prb["SECTORCARRIERREF"] == sc["SECTORCARRIERREF"]), 
    how='left'
).withColumnRenamed("SECTOR", "EUTRANCELL") \
 .drop(sc["ENODEB"], sc["SECTORCARRIERREF"], sc["DAY"])

# Prepare transformation for Samsung/Nokia rows
vendor_condition = col("Vendor").isin("Samsung", "Nokia")
split_col = split(col("SECTORCARRIERREF"), "-")

df_hourly_prb = df_hourly_prb \
    .withColumn("EUTRANCELL", when(vendor_condition, split_col[0]).otherwise(col("EUTRANCELL"))) \
    .withColumn("CARRIER", when(vendor_condition, split_col[1]).otherwise(col("CARRIER")))

# Drop nulls from main dataframe
df_hrly_interference_all = df_hrly_interference_all.na.drop(subset=["EUTRANCELL", "CARRIER"])

# Atoll data join
df_hrly_interference_all = df_hrly_interference_all.join(
    tb,
    (df_hrly_interference_all.ENODEB == tb.ENODEB_P) &
    (df_hrly_interference_all.EUTRANCELL == tb.SECTOR_P) &
    (df_hrly_interference_all.CARRIER == tb.CARRIER_P),
    how="left"
).drop("ENODEB_P", "SECTOR_P", "CARRIER_P")

# Filter weak interference
df_hrly_interference_all = df_hrly_interference_all.filter(
    col("WEIGHTEDAVGROIDINTERFERENCE").between(-140, -110)
)

# Vendor filter
vendor_condition = col("Vendor").isin("Samsung", "Nokia")
df_hrly_interference_all = df_hrly_interference_all.filter(vendor_condition)

# Optional PIM filter
# df_hrly_interference_all = df_hrly_interference_all.filter(
#     (col("Vendor") == "Ericsson") & (col("PIM_MAX").isNull())
# )

# Neighbor + freq join helpers
def join_with_neighbors(df_cr, get_neighbors_fn, get_freq_fn, spark, key_cols):
    neighbors = get_neighbors_fn(spark).dropDuplicates(key_cols)
    freq = get_freq_fn(spark).dropDuplicates(key_cols)
    return df_cr.join(neighbors, on=key_cols, how="left_semi") \
                .join(freq, on=key_cols, how="left_semi")

sam = join_with_neighbors(df_cr, get_cell_relation_samsung, get_freq_data_samsung, spark, ["EUTRANCELL"])
nokia = join_with_neighbors(df_cr, get_cell_relation_nokia, get_freq_data_nokia, spark, ["EUTRANCELL"])

nw

# Add LTECELLID column based on sector + carrier values – needed for receiving PRB data
df_cr = df_cr.withColumn(
    "LTECELLID",
    when(
        col("CARRIER") == '1',
        (col("EUTRANCELL").cast("int") + lit(0)).cast("string")
    ).otherwise(col("EUTRANCELL").cast("string"))
)

# Create a dataframe of master enodeb sector carriers with interference – needed to filter PRB values
df_intf = df_cr.select("ENODEB", "EUTRANCELL", "CARRIER").dropDuplicates()

# Join hourly PRB with interference dataframe
df_hourly_prb_join = (
    df_hourly_prb
    .join(df_intf, 
          (df_hourly_prb.ENODEB == df_intf.ENODEB) &
          (df_hourly_prb.EUTRANCELL == df_intf.EUTRANCELL) &
          (df_hourly_prb.CARRIER == df_intf.CARRIER),
          "inner")
)

# Drop unwanted columns and rename for clarity
df_hourly_prb = (
    df_hourly_prb_join
    .drop("SECTORCARRIERREF", "trans_dt")
    .withColumnRenamed("day_copy", "WEIGHTEDAVGGRADATIONINTERFERENCE")
)

# Calculate PRB daily average dBm
df_prb_avg_dBm = calc_prb_avg_dBm(df_hourly_prb)

# Calculate PRB daily count
df_prb_count = calc_prb_daily_count(df_hourly_prb)

# Join max PRBs
df_max_prbs = df_max_prbs.join(df_prb_avg_dBm, on=["ENODEB", "EUTRANCELL", "CARRIER", "VENDOR"])

# Reorder columns
df_max_prbs = df_max_prbs.orderBy("ENODEB")

# Extract relevant PRB columns
cols = [col for col in df_max_prbs.columns if "radiocenterfprwprbprb" in col]

# Create key_avg column
df_max_prbs = df_max_prbs.withColumn(
    "key_avg",
    F.coalesce(
        *[F.when(
            df_max_prbs.key == c.split("_")[0].split("prb")[1], 
            df_max_prbs[c]
        ) for c in cols]
    )
)

# Loop to drop unused columns
base = "radiocenterfprwprbprb"
for i in range(1, 101):
    df_max_prbs = df_max_prbs.drop(f"{base}{i}_dBm")

# Select only macro sectors whose avg is >= -110
df_max_prbs = df_max_prbs.filter(col("key_avg") >= -110)




