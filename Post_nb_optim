from pyspark.sql.functions import col, lit

def get_narrowband_sectors(prb_vision_start, prb_vision_end, alptCoreDB, elptCoreDB, interf_level, sam_data_present, spark, markets=[]):
    sam_sectors = None

    if sam_data_present:
        sam_sectors = get_hot_samsung_sectors_sparkdf(prb_vision_start, prb_vision_end, alptCoreDB, interf_level, markets)
        broken_sam = get_hot_samsung_sectors_sparkdf(prb_vision_start, prb_vision_end, elptCoreDB, interf_level, markets)
    else:
        broken_sam = spark.createDataFrame([], get_hot_samsung_sectors_sparkdf.schema)  # or use an empty DataFrame with same schema

    broken_nok = get_hot_nokia_sectors_sparkdf(prb_vision_start, prb_vision_end, alptCoreDB, interf_level, markets)
    broken_eric = get_hot_ericsson_sectors_sparkdf(prb_vision_start, prb_vision_end, elptCoreDB, interf_level, markets)

    broken_branches = broken_sam.union(broken_nok).union(broken_eric)

    # Add VENDOR column for each and union
    eric_sectors = broken_eric.withColumn("VENDOR", lit("Ericsson"))
    vendor_sectors = [eric_sectors]

    if sam_data_present:
        sam_sectors = sam_sectors.withColumn("VENDOR", lit("Samsung")).withColumn("rfbranchrx", lit(1))
        vendor_sectors.append(sam_sectors)

    nok_sectors = broken_nok.withColumn("VENDOR", lit("Nokia"))
    vendor_sectors.append(nok_sectors)

    # Union all vendor sectors
    all_sectors = vendor_sectors[0]
    for sect in vendor_sectors[1:]:
        all_sectors = all_sectors.union(sect)

    all_sectors = all_sectors.withColumn("day_copy", col("day")) \
                             .withColumn("hr", col("hr")) \
                             .dropDuplicates(subset=["ENODEB", "SECTORCARRIERREF"])

    # Scoring
    sector_ref_copy = all_sectors.select("branchrx", "day", "hr").withColumnRenamed("branchrx", "secbranchrx")
    df_predictions = all_sectors.drop("day_copy", "SECTORCARRIERREF", "VENDOR") \
        .join(sector_ref_copy, on=["ENODEB", "hr", "day"]) \
        .applyInPandas(create_and_score_prb_image, schema=narrowband_schema)

    # Efficient casting
    cols_to_cast = ["nb", "nb_count", "hr", "nb_hour", "nb_pred"]
    for col_name in cols_to_cast:
        df_predictions = df_predictions.withColumn(col_name, col(col_name).cast("int"))

    df_predictions = df_predictions.orderBy("SECTORCARRIERREF", "ENODEB", "end_hour", "nb_count", ascending=False) \
                                   .dropDuplicates(subset=["ENODEB", "SECTORCARRIERREF", "VENDOR", "day", "hr"])

    return all_sectors, df_predictions, broken_branches


df_predictions = (
    df_predictions
    .withColumn("nb", col("nb") == "True")  # original logic: create boolean column
    .withColumn("start_hour", col("start_hour").cast("int"))
    .withColumn("end_hour", col("end_hour").cast("int"))
    .withColumn("start_prb", col("start_prb").cast("int"))
    .withColumn("end_prb", col("end_prb").cast("int"))
    .withColumn("nb_count", col("nb_count").cast("int"))
    .select("SECTORCARRIERREF", "ENODEB", "start_hour", "end_hour", "start_prb", "end_prb", "nb", "nb_count")
    .dropDuplicates(["ENODEB", "SECTORCARRIERREF", "VENDOR", "start_hour", "end_hour", "start_prb", "end_prb", "nb"])
)

new 

# Line 275 onward â€” keep logic same, just optimize structure

if sam_sectors is not None:
    sam_sectors = sam_sectors.withColumn("VENDOR", lit("Samsung")) \
                             .withColumn("frbbranchr", lit(""))

    nok_sectors = nok_sectors.withColumn("VENDOR", lit("Nokia")) \
                             .withColumn("frbbranchr", lit(""))

    all_sectors = nok_sectors.select(erc_cols).union(sam_sectors.select(erc_cols)).union(all_sectors)

all_sectors = all_sectors.cache()

# Add helper columns
all_sectors = all_sectors.withColumns({
    "day_copy": col("day"),
    "seccarr_ref_copy": col("SECTORCARRIERREF")
})

# Drop duplicates and unnecessary column before scoring
all_sectors = all_sectors.dropDuplicates(["ENODEB", "SECTORCARRIERREF", "VENDOR", "frbbranchr", "day", "hr"]) \
                         .drop("WEIGHTEDAVGRADIOINTERFERENCE")

# Score using pandas UDF
df_predictions = all_sectors.groupby("day_copy", "VENDOR", "frbbranchr", "seccarr_ref_copy") \
                            .applyInPandas(create_and_score_prb_image, schema=narrowband_schema)

# Filter for nb == 'True' and cast types
df_predictions = df_predictions.filter(col("nb") == "True") \
    .withColumn("nb_count", col("nb_count").cast("int")) \
    .withColumn("start_hour", col("start_hour").cast("int")) \
    .withColumn("end_hour", col("end_hour").cast("int")) \
    .withColumn("start_prb", col("start_prb").cast("int")) \
    .withColumn("end_prb", col("end_prb").cast("int"))

# Final formatting and deduplication
df_predictions = df_predictions.select(
    "SECTORCARRIERREF", "ENODEB", "start_hour", "end_hour",
    "start_prb", "end_prb", "nb", "nb_count"
).orderBy("start_hour", "end_hour", "start_prb", "end_prb", ascending=False) \
 .dropDuplicates(["ENODEB", "SECTORCARRIERREF", "VENDOR"])

# Drop temporary columns from all_sectors
all_sectors = all_sectors.drop("day_copy", "seccarr_ref_copy")

# Return final results
return all_sectors, df_predictions, broken_branches

